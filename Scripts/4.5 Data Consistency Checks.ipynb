{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e186a12b-8de9-42c6-827c-0298d7775d5a",
   "metadata": {},
   "source": [
    "# Task 4.5: Data Consistency Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b9d18-60a8-4079-be59-c7e6678e6a19",
   "metadata": {},
   "source": [
    "# Data Consistency Checks - Task 4.5\n",
    "\n",
    "This notebook performs comprehensive data consistency checks on the Instacart project datasets.\n",
    "We will examine the orders and products data for:\n",
    "- Mixed data types\n",
    "- Missing values  \n",
    "- Duplicate records\n",
    "- Data quality issues\n",
    "\n",
    "The focus will be on the df_ords dataframe as specified in the task requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec2274-b3d0-4412-b797-68cb3a796359",
   "metadata": {},
   "source": [
    "# Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecca22af-9ecc-4675-8a16-dba882a8c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully imported!\n",
      "Products dataframe shape: (49693, 5)\n",
      "Orders dataframe shape: (3421083, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set up project path\n",
    "path = r'/Users/josephadamski/Instacart Basket Analysis'\n",
    "\n",
    "# Import dataframes\n",
    "df_prods = pd.read_csv(os.path.join(path, 'Data', 'Original Data', 'products.csv'), index_col=False)\n",
    "df_ords = pd.read_csv(os.path.join(path, 'Data', 'Prepared Data', 'orders_wrangled.csv'), index_col=False)\n",
    "\n",
    "print(\"Data successfully imported!\")\n",
    "print(f\"Products dataframe shape: {df_prods.shape}\")\n",
    "print(f\"Orders dataframe shape: {df_ords.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da258a2-bb2a-4103-8bd0-32e4ba208433",
   "metadata": {},
   "source": [
    "## STEP 1: CONSISTENCY CHECKS ON df_prods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d59e8a-84d7-4b43-9ae7-adc3fa029c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING df_prods FOR MIXED TYPES ===\n",
      "Mixed types found in column: product_name\n",
      "\n",
      "=== MISSING VALUES IN df_prods ===\n",
      "product_id        0\n",
      "product_name     16\n",
      "aisle_id          0\n",
      "department_id     0\n",
      "prices            0\n",
      "dtype: int64\n",
      "\n",
      "=== DUPLICATES IN df_prods ===\n",
      "Number of duplicate rows: 5\n",
      "Sample duplicates:\n",
      "       product_id                                       product_name  \\\n",
      "462           462                  Fiber 4g Gummy Dietary Supplement   \n",
      "18459       18458                                         Ranger IPA   \n",
      "26810       26808               Black House Coffee Roasty Stout Beer   \n",
      "35309       35306  Gluten Free Organic Peanut Butter & Chocolate ...   \n",
      "35495       35491                            Adore Forever Body Wash   \n",
      "\n",
      "       aisle_id  department_id  prices  \n",
      "462          70             11     4.8  \n",
      "18459        27              5     9.2  \n",
      "26810        27              5    13.4  \n",
      "35309       121             14     6.8  \n",
      "35495       127             11     9.9  \n",
      "\n",
      "Cleaned df_prods shape: (49672, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CHECKING df_prods FOR MIXED TYPES ===\")\n",
    "mixed_found = False\n",
    "for col in df_prods.columns.tolist():\n",
    "    weird = (df_prods[[col]].map(type) != df_prods[[col]].iloc[0].apply(type)).any(axis=1)\n",
    "    if len(df_prods[weird]) > 0:\n",
    "        print(f\"Mixed types found in column: {col}\")\n",
    "        mixed_found = True\n",
    "\n",
    "if not mixed_found:\n",
    "    print(\"No mixed data types found in df_prods!\")\n",
    "\n",
    "print(\"\\n=== MISSING VALUES IN df_prods ===\")\n",
    "prods_missing = df_prods.isnull().sum()\n",
    "print(prods_missing)\n",
    "\n",
    "print(\"\\n=== DUPLICATES IN df_prods ===\")\n",
    "df_prods_dups = df_prods[df_prods.duplicated()]\n",
    "print(f\"Number of duplicate rows: {len(df_prods_dups)}\")\n",
    "if len(df_prods_dups) > 0:\n",
    "    print(\"Sample duplicates:\")\n",
    "    print(df_prods_dups.head())\n",
    "\n",
    "# Clean df_prods for later export\n",
    "df_prods_clean = df_prods[df_prods['product_name'].notna()].drop_duplicates()\n",
    "print(f\"\\nCleaned df_prods shape: {df_prods_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42adc4ae-ecc8-4035-ab21-6a495c73646f",
   "metadata": {},
   "source": [
    "### Step 1 Complete - df_prods Consistency Checks\n",
    "**Mixed Types:** Mixed data types found in product_name column\n",
    "**Missing Values:** 16 missing values found in product_name column  \n",
    "**Duplicates:** 5 duplicate records found\n",
    " \n",
    "The missing values and duplicates have been cleaned for the final export version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd56a8-239c-4865-811c-3df3da76b5f7",
   "metadata": {},
   "source": [
    "## STEP 2: DESCRIPTIVE STATISTICS FOR df_ords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cadaa58f-85e2-497b-9346-ca5ad9a40244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DESCRIPTIVE STATISTICS FOR df_ords ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>orders_day_of_week</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>3.214874e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.710542e+06</td>\n",
       "      <td>1.029782e+05</td>\n",
       "      <td>1.715486e+01</td>\n",
       "      <td>2.776219e+00</td>\n",
       "      <td>1.345202e+01</td>\n",
       "      <td>1.111484e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.875817e+05</td>\n",
       "      <td>5.953372e+04</td>\n",
       "      <td>1.773316e+01</td>\n",
       "      <td>2.046829e+00</td>\n",
       "      <td>4.226088e+00</td>\n",
       "      <td>9.206737e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.552715e+05</td>\n",
       "      <td>5.139400e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.710542e+06</td>\n",
       "      <td>1.026890e+05</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.565812e+06</td>\n",
       "      <td>1.543850e+05</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>2.062090e+05</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           order_id       user_id  order_number  orders_day_of_week  \\\n",
       "count  3.421083e+06  3.421083e+06  3.421083e+06        3.421083e+06   \n",
       "mean   1.710542e+06  1.029782e+05  1.715486e+01        2.776219e+00   \n",
       "std    9.875817e+05  5.953372e+04  1.773316e+01        2.046829e+00   \n",
       "min    1.000000e+00  1.000000e+00  1.000000e+00        0.000000e+00   \n",
       "25%    8.552715e+05  5.139400e+04  5.000000e+00        1.000000e+00   \n",
       "50%    1.710542e+06  1.026890e+05  1.100000e+01        3.000000e+00   \n",
       "75%    2.565812e+06  1.543850e+05  2.300000e+01        5.000000e+00   \n",
       "max    3.421083e+06  2.062090e+05  1.000000e+02        6.000000e+00   \n",
       "\n",
       "       order_hour_of_day  days_since_prior_order  \n",
       "count       3.421083e+06            3.214874e+06  \n",
       "mean        1.345202e+01            1.111484e+01  \n",
       "std         4.226088e+00            9.206737e+00  \n",
       "min         0.000000e+00            0.000000e+00  \n",
       "25%         1.000000e+01            4.000000e+00  \n",
       "50%         1.300000e+01            7.000000e+00  \n",
       "75%         1.600000e+01            1.500000e+01  \n",
       "max         2.300000e+01            3.000000e+01  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== DESCRIPTIVE STATISTICS FOR df_ords ===\")\n",
    "df_ords.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b6b45-34b6-413d-a0f9-60025a2601c4",
   "metadata": {},
   "source": [
    "### Step 2 - Analysis of df_ords Descriptive Statistics\n",
    " \n",
    "From the df.describe() output, I can observe the following about the data quality:\n",
    "\n",
    "**Orders Day of Week (orders_day_of_week):**\n",
    "- Range: 0-6, which correctly represents the 7 days of the week\n",
    "- Mean of ~2.8 suggests fairly even distribution across weekdays\n",
    " \n",
    "**Order Hour of Day (order_hour_of_day):** \n",
    "- Range: 0-23, which correctly represents the 24-hour format\n",
    "- No values exceed 23, indicating clean time data\n",
    " \n",
    "**Days Since Prior Order:**\n",
    "- The count (3,214,874) is lower than other columns (3,421,083)\n",
    "- This suggests approximately 206,209 missing values\n",
    "- Missing values likely represent first-time customers who have no prior orders\n",
    " \n",
    "**Order Numbers:**\n",
    "- Maximum of 100 orders per customer seems reasonable\n",
    "- Minimum of 1 confirms all customers have at least one order\n",
    " \n",
    "**Overall Assessment:** The data ranges appear logical and within expected parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd370f25-f814-4ab3-99f7-10146a85201f",
   "metadata": {},
   "source": [
    "## STEP 3: CHECK FOR MIXED-TYPE DATA IN df_ords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d97ee380-cdcc-402d-a220-3138fff86edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHECKING df_ords FOR MIXED DATA TYPES ===\n",
      "No mixed data types found in df_ords!\n",
      "\n",
      "=== DATA TYPES IN df_ords ===\n",
      "order_id                    int64\n",
      "user_id                     int64\n",
      "order_number                int64\n",
      "orders_day_of_week          int64\n",
      "order_hour_of_day           int64\n",
      "days_since_prior_order    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CHECKING df_ords FOR MIXED DATA TYPES ===\")\n",
    "\n",
    "mixed_type_columns = []\n",
    "for col in df_ords.columns.tolist():\n",
    "    weird = (df_ords[[col]].map(type) != df_ords[[col]].iloc[0].apply(type)).any(axis=1)\n",
    "    if len(df_ords[weird]) > 0:\n",
    "        print(f\"Mixed types found in column: {col}\")\n",
    "        mixed_type_columns.append(col)\n",
    "\n",
    "if len(mixed_type_columns) == 0:\n",
    "    print(\"No mixed data types found in df_ords!\")\n",
    "\n",
    "print(\"\\n=== DATA TYPES IN df_ords ===\")\n",
    "print(df_ords.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5be6ba-b155-4dfc-8176-a34b8aba5597",
   "metadata": {},
   "source": [
    "### Step 3 - Mixed Data Type Analysis\n",
    "\n",
    "**Results:** No mixed data types were detected in the df_ords dataframe.\n",
    " \n",
    "**Data Type Summary:**\n",
    "- All ID columns (order_id, user_id) are properly stored as int64\n",
    "- Categorical variables (orders_day_of_week, order_hour_of_day) are int64\n",
    "- days_since_prior_order is float64 (appropriate due to potential missing values)\n",
    " \n",
    "**Conclusion:** The data types are consistent and appropriate for each column. No action needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841113b-ed49-44a7-a672-e78078610993",
   "metadata": {},
   "source": [
    "## STEP 4: FIX MIXED-TYPE DATA (IF FOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "421a87f5-e1e3-4e44-83fc-961e913c1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mixed-type data to fix in df_ords\n"
     ]
    }
   ],
   "source": [
    "# Since no mixed-type data was found, no fixes are needed\n",
    "# If mixed types were found, we would use:\n",
    "# df_ords['column_name'] = df_ords['column_name'].astype('desired_type')\n",
    "\n",
    "print(\"No mixed-type data to fix in df_ords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b3d8c-7a63-41fa-959d-fc1f8ed7857f",
   "metadata": {},
   "source": [
    "## STEP 5: CHECK FOR MISSING VALUES IN df_ords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28aa76d-f06d-4514-9c6d-d9bcd03caa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING VALUES ANALYSIS IN df_ords ===\n",
      "order_id                       0\n",
      "user_id                        0\n",
      "order_number                   0\n",
      "orders_day_of_week             0\n",
      "order_hour_of_day              0\n",
      "days_since_prior_order    206209\n",
      "dtype: int64\n",
      "\n",
      "Sample of records with missing days_since_prior_order:\n",
      "    order_id  user_id  order_number  days_since_prior_order\n",
      "0    2539329        1             1                     NaN\n",
      "11   2168274        2             1                     NaN\n",
      "26   1374495        3             1                     NaN\n",
      "39   3343014        4             1                     NaN\n",
      "45   2717275        5             1                     NaN\n",
      "50   2086598        6             1                     NaN\n",
      "54   2565571        7             1                     NaN\n",
      "75    600894        8             1                     NaN\n",
      "79    280530        9             1                     NaN\n",
      "83   1224907       10             1                     NaN\n",
      "\n",
      "Order numbers for records with missing days_since_prior_order:\n",
      "order_number\n",
      "1    206209\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== MISSING VALUES ANALYSIS IN df_ords ===\")\n",
    "missing_values = df_ords.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Examine the missing values in detail\n",
    "df_missing_analysis = df_ords[df_ords['days_since_prior_order'].isnull()]\n",
    "print(f\"\\nSample of records with missing days_since_prior_order:\")\n",
    "print(df_missing_analysis[['order_id', 'user_id', 'order_number', 'days_since_prior_order']].head(10))\n",
    "\n",
    "# Check if missing values correspond to first orders\n",
    "first_order_check = df_missing_analysis['order_number'].value_counts().sort_index()\n",
    "print(f\"\\nOrder numbers for records with missing days_since_prior_order:\")\n",
    "print(first_order_check.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a502b-cd90-4ff2-a76b-cca0bfa5ec7d",
   "metadata": {},
   "source": [
    "### Step 5 - Missing Values Analysis\n",
    " \n",
    "**Findings:**\n",
    "- 206,209 missing values found in the 'days_since_prior_order' column (6.03% of total records)\n",
    "- All other columns have complete data\n",
    " \n",
    "**Root Cause Analysis:**\n",
    "Upon examination, ALL missing values correspond to records where order_number = 1.\n",
    "This makes logical sense because first-time customers have no previous orders,\n",
    "so there are no \"days since prior order\" to calculate.\n",
    " \n",
    "**Proposed Solution:**\n",
    "Create a flag to identify first-time orders while preserving the data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9f96c-4e22-4d93-81bb-211a6f0e09f9",
   "metadata": {},
   "source": [
    "## STEP 6: ADDRESS MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a940620-5ae1-4b27-ad53-08a1026ea066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ADDRESSING MISSING VALUES ===\n",
      "First Order Flag Summary:\n",
      "First-time orders (True): 206,209\n",
      "Repeat orders (False): 3,214,874\n",
      "\n",
      "Verification: All order_number = 1 have first_order_flag = True: True\n",
      "Dataframe shape after adding flag: (3421083, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ADDRESSING MISSING VALUES ===\")\n",
    "\n",
    "# Create a flag for first-time orders\n",
    "df_ords['first_order_flag'] = df_ords['days_since_prior_order'].isnull()\n",
    "\n",
    "# Verify the flag works correctly\n",
    "first_order_summary = df_ords['first_order_flag'].value_counts()\n",
    "print(\"First Order Flag Summary:\")\n",
    "print(f\"First-time orders (True): {first_order_summary[True]:,}\")\n",
    "print(f\"Repeat orders (False): {first_order_summary[False]:,}\")\n",
    "\n",
    "# Cross-check with order_number = 1\n",
    "verification = df_ords[df_ords['order_number'] == 1]['first_order_flag'].all()\n",
    "print(f\"\\nVerification: All order_number = 1 have first_order_flag = True: {verification}\")\n",
    "\n",
    "print(f\"Dataframe shape after adding flag: {df_ords.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1442ff-f781-4e0c-b3ba-b309fa953e77",
   "metadata": {},
   "source": [
    "### Step 6 - Missing Values Resolution\n",
    " \n",
    "**Method Used:** Created a boolean flag 'first_order_flag' to identify first-time customers\n",
    "\n",
    "**Why I chose this method:** I created a flag instead of dropping records because the missing \n",
    "values represent legitimate business cases (first-time customers). Dropping these records would \n",
    "lose valuable customer data and reduce our dataset size unnecessarily. This approach preserves \n",
    "all data while enabling customer segmentation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03893b8-5628-4563-aca7-0f24b4881ec0",
   "metadata": {},
   "source": [
    "## STEP 7: CHECK FOR DUPLICATE VALUES IN df_ords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4332cbcd-cee1-456d-b462-c7d7a1c647d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHECKING FOR DUPLICATE RECORDS ===\n",
      "Number of duplicate rows found: 0\n",
      "No duplicate records found in df_ords!\n",
      "\n",
      "=== CHECKING KEY COLUMN UNIQUENESS ===\n",
      "Unique order_ids: 3,421,083 (should equal total rows)\n",
      "Total rows: 3,421,083\n",
      "Order_id uniqueness: PASS\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CHECKING FOR DUPLICATE RECORDS ===\")\n",
    "\n",
    "df_ords_duplicates = df_ords[df_ords.duplicated()]\n",
    "duplicate_count = len(df_ords_duplicates)\n",
    "\n",
    "print(f\"Number of duplicate rows found: {duplicate_count:,}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nSample of duplicate records:\")\n",
    "    print(df_ords_duplicates.head())\n",
    "else:\n",
    "    print(\"No duplicate records found in df_ords!\")\n",
    "\n",
    "# Check for duplicates in key identifier columns\n",
    "print(\"\\n=== CHECKING KEY COLUMN UNIQUENESS ===\")\n",
    "print(f\"Unique order_ids: {df_ords['order_id'].nunique():,} (should equal total rows)\")\n",
    "print(f\"Total rows: {len(df_ords):,}\")\n",
    "print(f\"Order_id uniqueness: {'PASS' if df_ords['order_id'].nunique() == len(df_ords) else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6cdbb-f7f2-4d7c-b53b-656665bf67d6",
   "metadata": {},
   "source": [
    "### Step 7 - Duplicate Records Analysis\n",
    " \n",
    "**Findings:** No complete duplicate records found in df_ords\n",
    " \n",
    "**Key Column Analysis:**\n",
    "- Order_id uniqueness: VERIFIED\n",
    "- Each order_id appears exactly once, confirming data integrity\n",
    "- Total unique order_ids matches total row count (3,421,083)\n",
    " \n",
    "**Assessment:** The dataset demonstrates excellent referential integrity with no duplicate transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bfd60-4d07-4768-bca7-096e1f508ef4",
   "metadata": {},
   "source": [
    "## STEP 8: ADDRESS DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb2e279c-f701-41e5-89d4-e97b350ebedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate records to remove from df_ords\n"
     ]
    }
   ],
   "source": [
    "# Since no duplicates were found, no action is needed\n",
    "print(\"No duplicate records to remove from df_ords\")\n",
    "\n",
    "# If duplicates had been found, we would use:\n",
    "# df_ords_clean = df_ords.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330202e-8497-4e70-aabd-784f0a91bd91",
   "metadata": {},
   "source": [
    "### Step 8 - Duplicate Handling\n",
    " \n",
    "**Method Used:** No action required as no duplicates were found\n",
    " \n",
    "**Why I chose this method:** Since no duplicates were found, no action was needed. \n",
    "If duplicates had been found, I would have used drop_duplicates() to maintain data \n",
    "integrity while preserving unique records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ff385-018f-4bcb-b20b-c6d5f1df0210",
   "metadata": {},
   "source": [
    "## STEP 9: EXPORT CLEANED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ce62ea9-1b9c-453d-a6ac-7eae920862b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPORTING CLEANED DATASETS ===\n",
      "Orders data exported: orders_checked.csv\n",
      "Products data exported: products_checked.csv\n",
      "\n",
      "FILES CREATED:\n",
      "- orders_checked.csv (3,421,083 records)\n",
      "- products_checked.csv (49,672 records)\n",
      "\n",
      "TASK 4.5 COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EXPORTING CLEANED DATASETS ===\")\n",
    "\n",
    "# Export the cleaned orders data with the new flag\n",
    "orders_export_path = os.path.join(path, 'Data', 'Prepared Data', 'orders_checked.csv')\n",
    "df_ords.to_csv(orders_export_path, index=False)\n",
    "print(f\"Orders data exported: orders_checked.csv\")\n",
    "\n",
    "# Export the cleaned products data  \n",
    "products_export_path = os.path.join(path, 'Data', 'Prepared Data', 'products_checked.csv')\n",
    "df_prods_clean.to_csv(products_export_path, index=False)\n",
    "print(f\"Products data exported: products_checked.csv\")\n",
    "\n",
    "print(f\"\\nFILES CREATED:\")\n",
    "print(f\"- orders_checked.csv ({len(df_ords):,} records)\")\n",
    "print(f\"- products_checked.csv ({len(df_prods_clean):,} records)\")\n",
    "\n",
    "print(f\"\\nTASK 4.5 COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
